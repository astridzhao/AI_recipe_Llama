{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "[Full instructions of Jupyter lab setup](https://github.com/TrelisResearch/install-guides/blob/main/jupyter-lab-setup.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you are using Apple Silicon (M1) Mac, make sure you have installed a version of Python that supports arm64 architecture; Otherwise, while installing it will build the llama.ccp x86 version which will be 10x slower on Apple Silicon (M1) Mac. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm64path = \"Miniforge3-MacOSX-arm64.sh\"\n",
    "if os.path.exists(arm64path):\n",
    "    print(\"Version of Python that supports arm64 architecture already exists!\")\n",
    "else:\n",
    "    print(\"Uncomment the next block of code and install python.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Llama.cpp (only need to do this once)\n",
    "The instructions below are for Macs with an M1 chip.\n",
    "For other operating systems, comment out those cells and get instructions [here](https://github.com/TrelisResearch/llamacpp-install-basics/blob/main/instructions.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model file\n",
    "model_name = 'TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf'\n",
    "pure_name = model_name.split('/')[-1]\n",
    "print(\"Pure name of model is: \", pure_name)\n",
    "\n",
    "parts = model_name.split('/')\n",
    "model_path = f\"{parts[0]}/{parts[1]}\"\n",
    "\n",
    "print(\"Model path is: \", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('llama.cpp'):\n",
    "    print(\"Cloning llama.cpp...\")\n",
    "    !git clone https://github.com/ggerganov/llama.cpp\n",
    "    %cd llama.cpp\n",
    "\n",
    "    print(\"Compiling for Mac with M1 chip...\")\n",
    "    !LLAMA_METAL=1 make\n",
    "    print(\"Compilation completed!\")\n",
    "            \n",
    "    %cd ../\n",
    "else:\n",
    "    print(\"llama.cpp has already been cloned into this directory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set directory to llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd llama.cpp\n",
    "\n",
    "if not os.path.exists(pure_name):\n",
    "    !wget https://huggingface.co/{model_name}\n",
    "else:\n",
    "    print(f\"{pure_name} already exists!\")\n",
    "\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default value for context_length to High Speed (4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 2048\n",
    "max_doc_length = int(0.75 * context_length)\n",
    "max_doc_tokens = max_doc_length\n",
    "n_predict = int(0.2 * context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import threading\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, clear_output, Markdown, FileLink\n",
    "import textwrap, json\n",
    "import ipywidgets as widgets\n",
    "import re, time\n",
    "import io\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "from functools import partial\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_wrapped(text):\n",
    "    # Regular expression pattern to detect code blocks\n",
    "    code_pattern = r'```(.+?)```'\n",
    "    matches = list(re.finditer(code_pattern, text, re.DOTALL))\n",
    "    if not matches:\n",
    "        # If there are no code blocks, display the entire text as Markdown\n",
    "        display(Markdown(text))\n",
    "        return\n",
    "    start = 0\n",
    "    for match in matches:\n",
    "        # Display the text before the code block as Markdown\n",
    "        before_code = text[start:match.start()].strip()\n",
    "        if before_code:\n",
    "            display(Markdown(before_code))\n",
    "        # Display the code block\n",
    "        code = match.group(0).strip()  # Extract code block\n",
    "        display(Markdown(code))  # Display code block\n",
    "        start = match.end()\n",
    "    # Display the text after the last code block as Markdown\n",
    "    after_code = text[start:].strip()  # Text after the last code block\n",
    "    if after_code:\n",
    "        display(Markdown(after_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = f\"\"\"You are a helpful recipe-generating assistant. Based on the following given ingredients, you will generate a recipe. Make sure to follow the rules listed below: 1. Please don't give a very long recipe (more than 1000 words), make the description in 500-1000 words. 2. Warn the user if there is any common allergies ingredients in your recipe. 3. If you will need to use any ingredients outside of the ingredients that the user provided, Warn the user. 4. Provide other essential information about the recipe such as kitchen utensils, preparation steps. 5. Choose some common spice/sauce first, unless the user provided a very specific sauce want to use. 6. The default serving size is 2, unless the user specifies. 7. The default dish style is American/Italian cuisine, unless the user specifies. 8. The default type of dish is airfry/oven/stir-fry, unless the user specifies. 9. Use both text and some cute emoji if you can. \"\"\"\n",
    "\n",
    "SYSTEM_PROMPT = DEFAULT_SYSTEM_PROMPT\n",
    "\n",
    "\n",
    "#initialize the dialog\n",
    "dialog_history = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "\n",
    "button = widgets.Button(description=\"Send\")\n",
    "\n",
    "usertext = widgets.Textarea(layout=widgets.Layout(width='800px'))\n",
    "\n",
    "output_log = widgets.Output()\n",
    "\n",
    "#-------------------------------------------->\n",
    "# Function to handle the subprocess output and update the dialog history\n",
    "def generate_response(process, output_widget):\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        #reinitialize assistant_response each time\n",
    "        if process.poll() is not None and output == '':\n",
    "            print(\"Subprocess has completed.\")\n",
    "            break \n",
    "        if output:\n",
    "            if '[INST]' or '<>' in output :\n",
    "                continue\n",
    "            if '[/INST]' in output:\n",
    "                inst_index = output.find('[/INST]')\n",
    "                # Check if [/INST] is found in the text\n",
    "                if inst_index != -1:\n",
    "                    # Print everything after [/INST]\n",
    "                    assistant_response = output[inst_index + len('[/INST]'):].strip()\n",
    "            else:\n",
    "                assistant_response = f\"{output.strip()}\"\n",
    "            \n",
    "            dialog_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "            \n",
    "            if assistant_response:\n",
    "                # Update the output widget\n",
    "                with output_widget:\n",
    "                    print_wrapped(f'{assistant_response}\\n')\n",
    "        else:\n",
    "            break\n",
    "    process.stdout.close()\n",
    "#-------------------------------------------->  \n",
    "\n",
    "#when the user start to use model\n",
    "def on_button_clicked(b):\n",
    "    user_input = usertext.value\n",
    "    dialog_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    usertext.value = ''\n",
    "\n",
    "    # Change button description and color, and disable it\n",
    "    button.description = 'Processing...'\n",
    "    button.style.button_color = '#ff6e00'  # Use hex color codes for better color choices\n",
    "    button.disabled = True  # Disable the button when processing\n",
    "\n",
    "    with output_log:\n",
    "        clear_output()\n",
    "        for message in dialog_history:\n",
    "            print_wrapped(f'**{message[\"role\"].capitalize()}**: {message[\"content\"]}\\n')\n",
    "\n",
    "    prompt_template = f'''[INST] <<SYS>>\n",
    "                        {SYSTEM_PROMPT}\n",
    "                        <</SYS>>\n",
    "                        {user_input} [/INST]'''\n",
    "    \n",
    "    # Start the subprocess and the threading to handle its output\n",
    "    if (os.getcwd() != \"/Users/astridz/Documents/AI_recipe/llama.cpp\"):\n",
    "        os.chdir('/Users/astridz/Documents/AI_recipe/llama.cpp')\n",
    "\n",
    "    pure_name = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "    args = ['./main', '-m', pure_name, '-c', '2048', '-ngl', '48', '-p', prompt_template]\n",
    "    process = subprocess.Popen(args, stdout=subprocess.PIPE, text=True)\n",
    "    # Start the thread that will handle the subprocess output\n",
    "    output_thread = threading.Thread(target=generate_response, args=(process,output_log))\n",
    "    output_thread.start()\n",
    "\n",
    "    # Wait for the subprocess and thread to finish\n",
    "    process.wait()\n",
    "    output_thread.join()\n",
    "\n",
    "    # Re-enable the button, reset description and color after processing\n",
    "    button.description = 'Send'\n",
    "    button.style.button_color = 'lightgray'\n",
    "    button.disabled = False\n",
    "    # os.getcwd() != \"/Users/astridz/Documents/AI_recipe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "button.on_click(on_button_clicked)\n",
    "\n",
    "alert_out = widgets.Output()\n",
    "\n",
    "clear_button = widgets.Button(description=\"Clear Chat\")\n",
    "text = widgets.Textarea(layout=widgets.Layout(width='800px'))\n",
    "\n",
    "quit_button = widgets.Button(description=\"Force Quit\")\n",
    "text = widgets.Textarea(layout=widgets.Layout(width='800px'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def on_clear_button_clicked(b):\n",
    "    # Clear the dialog history\n",
    "    dialog_history.clear()\n",
    "    # Add back the initial system prompt\n",
    "    dialog_history.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
    "    # Clear the output log\n",
    "    with output_log:\n",
    "        clear_output()\n",
    "        \n",
    "clear_button.on_click(on_clear_button_clicked)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from ipywidgets import HBox, VBox\n",
    "\n",
    "# Create the title with HTML\n",
    "title = f\"<h1 style='color: #ff6e00;'>Jupyter Recipe Llama ðŸ¦™ ðŸ’»</h1> <p> Enter your ingredients! </p>\"\n",
    "\n",
    "# Assuming that output_log, alert_out, and text are other widgets or display elements...\n",
    "first_row = HBox([button, clear_button, quit_button])  # Arrange these buttons horizontally\n",
    "\n",
    "# Arrange the two rows of buttons and other display elements vertically\n",
    "layout = VBox([output_log, alert_out, usertext, first_row])\n",
    "\n",
    "display(HTML(title))  # Use HTML function to display the title\n",
    "display(layout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
