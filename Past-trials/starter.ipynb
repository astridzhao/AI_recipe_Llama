{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/astridz/Documents/AI_recipe/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "from ctransformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "dlsym(0x200775bc0, llama_model_default_params): symbol not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/astridz/Documents/AI_recipe/starter.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/astridz/Documents/AI_recipe/starter.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_cpp\u001b[39;00m \u001b[39mimport\u001b[39;00m Llama\n",
      "File \u001b[0;32m~/Documents/AI_recipe/venv/lib/python3.10/site-packages/llama_cpp/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mllama_cpp\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mllama\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      4\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.2.11\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/AI_recipe/venv/lib/python3.10/site-packages/llama_cpp/llama_cpp.py:448\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mllama_model_default_params\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m llama_model_params:\n\u001b[1;32m    445\u001b[0m     \u001b[39mreturn\u001b[39;00m _lib\u001b[39m.\u001b[39mllama_model_default_params()\n\u001b[0;32m--> 448\u001b[0m _lib\u001b[39m.\u001b[39;49mllama_model_default_params\u001b[39m.\u001b[39margtypes \u001b[39m=\u001b[39m []\n\u001b[1;32m    449\u001b[0m _lib\u001b[39m.\u001b[39mllama_model_default_params\u001b[39m.\u001b[39mrestype \u001b[39m=\u001b[39m llama_model_params\n\u001b[1;32m    452\u001b[0m \u001b[39m# LLAMA_API struct llama_context_params llama_context_default_params(void);\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ctypes/__init__.py:387\u001b[0m, in \u001b[0;36mCDLL.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39m__\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m name\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    386\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(name)\n\u001b[0;32m--> 387\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(name)\n\u001b[1;32m    388\u001b[0m \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, name, func)\n\u001b[1;32m    389\u001b[0m \u001b[39mreturn\u001b[39;00m func\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ctypes/__init__.py:392\u001b[0m, in \u001b[0;36mCDLL.__getitem__\u001b[0;34m(self, name_or_ordinal)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, name_or_ordinal):\n\u001b[0;32m--> 392\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_FuncPtr((name_or_ordinal, \u001b[39mself\u001b[39;49m))\n\u001b[1;32m    393\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(name_or_ordinal, \u001b[39mint\u001b[39m):\n\u001b[1;32m    394\u001b[0m         func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m=\u001b[39m name_or_ordinal\n",
      "\u001b[0;31mAttributeError\u001b[0m: dlsym(0x200775bc0, llama_model_default_params): symbol not found"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method1: Langchain Framework with LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "    llm = AutoModelForCausalLM.from_pretrained(\n",
    "        \"./Llama/llama-2-7b.Q4_K_M.gguf\", \n",
    "        model_type= 'Llama',\n",
    "        temperature = 0.9\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "llm = load_llm()\n",
    "# response = llm(\"who is the author of harry potter?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"The following is a friendly conversation between a user and an AI. \n",
    "The AI is talkative and provides detailed answer for each question. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "\n",
    "User: Tell me a recipe based on {ingredient} by using one of following methods: {utensil}. \\n\n",
    "\n",
    "AI: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"ingredient\", \"utensil\"], template=template)\n",
    "\n",
    "# conversation = ConversationChain(\n",
    "#     prompt=PROMPT,\n",
    "#     llm=load_llm(),\n",
    "#     verbose=True,\n",
    "#     memory=ConversationBufferMemory(ai_prefix=\"AI Assistant\"),\n",
    "# )\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(prompt=PROMPT , llm = llm)\n",
    "llm_chain.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Variables\n",
    "# DIR = \"/Users/astridz/Documents/Development/Llama\"\n",
    "# MODEL_ID = \"llama-2-7b\"\n",
    "# QUANTIZATION_METHODS = [\"Q4_K_M\", \"Q5_K_M\"] #\"q4_k_m\" or \"q5\"\n",
    "# QUANTIZATION_DEFAULT = \"Q4_K_M\"\n",
    "# # Constants\n",
    "# MODEL_NAME = MODEL_ID.split('/')[-1]\n",
    "# GGML_VERSION = \"gguf\"\n",
    "\n",
    "# # Convert to fp16: Converting a model to use float16 instead of float32 \n",
    "# # can decrease the model size (up to half) and improve performance on some GPUs. \n",
    "# fp16 = f\"{DIR}/{MODEL_NAME}.{QUANTIZATION_DEFAULT}.{GGML_VERSION}.fp16.bin\"\n",
    "\n",
    "# print(fp16)\n",
    "\n",
    "# !python /Users/astridz/Documents/Development/llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quantize the model for each method in the QUANTIZATION_METHODS list\n",
    "# for method in QUANTIZATION_METHODS:\n",
    "#     qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{GGML_VERSION}.{method}.bin\"\n",
    "#     !./llama.cpp/quantize {fp16} {qtype} {method}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our two quantized models are now ready for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use llama.cpp to efficiently run them. we’ll use the -ngl 35 parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_list = [file for file in os.listdir(MODEL_NAME) if GGML_VERSION in file]\n",
    "\n",
    "# prompt = input(\"Enter your prompt: \")\n",
    "# chosen_method = input(\"Please specify the quantization method to run the model (options: \" + \", \".join(model_list) + \"): \")\n",
    "\n",
    "# #input a prompt\n",
    "# if chosen_method not in model_list:\n",
    "#     print(\"Invalid method chosen!\")\n",
    "# else:\n",
    "#     qtype = f\"{DIR}/{MODEL_NAME}/{MODEL_NAME.lower()}.{GGML_VERSION}.{method}.bin\"\n",
    "#     !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bring in deps\n",
    "# import streamlit as st \n",
    "# from langchain.llms import LlamaCpp\n",
    "# from langchain.embeddings import LlamaCppEmbeddings\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain.chains import LLMChain\n",
    "# from langchain.document_loaders import TextLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain.chains import ConversationChain\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "# # from threading import lock\n",
    "# from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "# from langchain.chains import LLMChain\n",
    "# from langchain.llms import LlamaCpp\n",
    "\n",
    "\n",
    "# # Customize the layout\n",
    "# st.set_page_config(page_title=\"DOCAI\", page_icon=\"🤖\", layout=\"wide\", )     \n",
    "# st.markdown(f\"\"\"\n",
    "#             <style>\n",
    "#             .stApp {{background-image: url(\"https://images.unsplash.com/photo-1509537257950-20f875b03669?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1469&q=80\"); \n",
    "#                      background-attachment: fixed;\n",
    "#                      background-size: cover}}\n",
    "#          </style>\n",
    "#          \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# # function for writing uploaded file in temp\n",
    "# def write_text_file(content, file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'w') as file:\n",
    "#             file.write(content)\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error occurred while writing the file: {e}\")\n",
    "#         return False\n",
    "\n",
    "# # set prompt template\n",
    "# prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# Answer:\"\"\"\n",
    "# prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# # initialize the LLM & Embeddings\n",
    "# llm = LlamaCpp(model_path=\"./models/llama-7b.ggmlv3.q4_0.bin\")\n",
    "# embeddings = LlamaCppEmbeddings(model_path=\"models/llama-7b.ggmlv3.q4_0.bin\")\n",
    "# llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# st.title(\"📄 Document Conversation 🤖\")\n",
    "# uploaded_file = st.file_uploader(\"Upload an article\", type=\"txt\")\n",
    "\n",
    "# if uploaded_file is not None:\n",
    "#     content = uploaded_file.read().decode('utf-8')\n",
    "#     # st.write(content)\n",
    "#     file_path = \"temp/file.txt\"\n",
    "#     write_text_file(content, file_path)   \n",
    "    \n",
    "#     loader = TextLoader(file_path)\n",
    "#     docs = loader.load()    \n",
    "#     text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "#     texts = text_splitter.split_documents(docs)\n",
    "#     db = Chroma.from_documents(texts, embeddings)    \n",
    "#     st.success(\"File Loaded Successfully!!\")\n",
    "    \n",
    "#     # Query through LLM    \n",
    "#     question = st.text_input(\"Ask something from the file\", placeholder=\"Find something similar to: ....this.... in the text?\", disabled=not uploaded_file,)    \n",
    "#     if question:\n",
    "#         similar_doc = db.similarity_search(question, k=1)\n",
    "#         context = similar_doc[0].page_content\n",
    "#         query_llm = LLMChain(llm=llm, prompt=prompt)\n",
    "#         response = query_llm.run({\"context\": context, \"question\": question})        \n",
    "#         st.write(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
